{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERTopic Model on Far-Right Telegram Data\n",
        "\n",
        "This notebook runs BERTopic on a filtered subset of channels from the dataset. It reads data from `data/`, filters by channel names in `docs/network-channel-names.csv`, fits a topic model, and writes results to `output/`.\n",
        "\n",
        "## Setup\n",
        "\n",
        "This project uses [uv](https://docs.astral.sh/uv/) to manage Python and dependencies. To run this notebook:\n",
        "\n",
        "1. Install `uv` from https://docs.astral.sh/uv/getting-started/installation/\n",
        "2. From the root of this repository, run: `uv run --with jupyter jupyter lab run-model.ipynb`\n",
        "\n",
        "`uv` will automatically install the correct Python version and all required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"1\"\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "import re\n",
        "import glob\n",
        "import pandas as pd\n",
        "from natsort import natsorted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clubnames_path = \"./docs/network-channel-names.csv\"\n",
        "clubnames_df = pd.read_csv(clubnames_path)\n",
        "clubnames_list = clubnames_df[\"clubname\"].dropna().tolist()\n",
        "print(f\">>> Loaded {len(clubnames_list)} club names.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "datasets_chunked = natsorted(glob.glob(\"./data/processed_part_*.csv\"))\n",
        "print(f\">>> Found {len(datasets_chunked)} data files (sorted numerically).\")\n",
        "\n",
        "channelnames: list[str] = []\n",
        "for f in datasets_chunked:\n",
        "    df = pd.read_csv(f, usecols=[\"channel_name\"])\n",
        "    channelnames.extend(df[\"channel_name\"].dropna().unique())\n",
        "\n",
        "channelname_df = pd.DataFrame({\"channel_name\": list(set(channelnames))})\n",
        "print(f\">>> Found {len(channelname_df)} unique channel names.\")\n",
        "\n",
        "save_path = \"./output/allchannelnames.csv\"\n",
        "channelname_df.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
        "print(f\">>> Saved channel names to {save_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "channelnames_list = channelname_df[\"channel_name\"].tolist()\n",
        "channelnames_pattern = r'\\b(?:' + '|'.join(map(re.escape, channelnames_list)) + r')\\b'\n",
        "\n",
        "docs: list[str] = []\n",
        "unique_docs: set[str] = set()\n",
        "\n",
        "print(\">>> Filtering and deduplicating documents...\")\n",
        "total = len(datasets_chunked)\n",
        "for i, dataset in enumerate(datasets_chunked, 1):\n",
        "    print(f\">>>   [{i}/{total}] {os.path.basename(dataset)}\")\n",
        "    try:\n",
        "        df_iter = pd.read_csv(\n",
        "            dataset, usecols=[\"channel_name\", \"cleaned_message\"],\n",
        "            sep=None, engine=\"python\", encoding=\"utf-8\", chunksize=10000,\n",
        "        )\n",
        "        for chunk in df_iter:\n",
        "            filtered = chunk[chunk[\"channel_name\"].isin(clubnames_df[\"clubname\"])].copy()\n",
        "            filtered = filtered.drop_duplicates(subset=[\"cleaned_message\"])\n",
        "            filtered.loc[:, \"cleaned_message\"] = (\n",
        "                filtered[\"cleaned_message\"].str.replace(channelnames_pattern, '', regex=True)\n",
        "            )\n",
        "            for msg in filtered[\"cleaned_message\"].dropna():\n",
        "                if msg not in unique_docs:\n",
        "                    unique_docs.add(msg)\n",
        "                    docs.append(msg)\n",
        "    except Exception as e:\n",
        "        print(f\">>>   Error processing {dataset}: {e}\")\n",
        "\n",
        "print(f\">>> Total unique documents collected: {len(docs)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "print(\">>> Loading embedding model (paraphrase-multilingual-MiniLM-L12-v2)...\")\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\">>> Encoding {len(docs)} documents (this may take a while)...\")\n",
        "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
        "print(\">>> Encoding complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\">>> Initializing UMAP, HDBSCAN, and CountVectorizer...\")\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=15, n_components=5,\n",
        "    min_dist=0.0, metric='cosine', random_state=42,\n",
        ")\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=150, metric='euclidean',\n",
        "    cluster_selection_method='eom', prediction_data=True,\n",
        "    core_dist_n_jobs=1,\n",
        ")\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words=\"english\", min_df=2, ngram_range=(1, 2),\n",
        ")\n",
        "\n",
        "print(\">>> Fitting BERTopic model...\")\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    top_n_words=20,\n",
        "    verbose=True,\n",
        ")\n",
        "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
        "\n",
        "n_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
        "print(f\">>> BERTopic fitting complete. Found {n_topics} topics.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\">>> Computing reduced embeddings for visualization...\")\n",
        "reduced_embeddings = UMAP(\n",
        "    n_neighbors=10, n_components=2,\n",
        "    min_dist=0.0, metric='cosine',\n",
        ").fit_transform(embeddings)\n",
        "print(\">>> Reduced embeddings complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\">>> Docs: {len(docs)} | Topics: {len(topics)} | Model topics: {len(topic_model.topics_)}\")\n",
        "\n",
        "df = pd.DataFrame({\"topic\": topics, \"document\": docs})\n",
        "save_path_df = \"./output/topic-model-results.csv\"\n",
        "df.to_csv(save_path_df, index=False, encoding=\"utf-8\")\n",
        "print(f\">>> Results saved to {save_path_df}\")\n",
        "print(\">>> Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}